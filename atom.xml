<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Steve Eichert]]></title>
  <link href="http://eichert12.github.com/atom.xml" rel="self"/>
  <link href="http://eichert12.github.com/"/>
  <updated>2012-02-09T21:36:47-05:00</updated>
  <id>http://eichert12.github.com/</id>
  <author>
    <name><![CDATA[Steve Eichert]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using Hadoop Streaming for XML processing]]></title>
    <link href="http://eichert12.github.com/hadoop,/mapreduce/2010/04/22/using-hadoop-streaming-for-xml-processing.html"/>
    <updated>2010-04-22T00:00:00-04:00</updated>
    <id>http://eichert12.github.com/hadoop,/mapreduce/2010/04/22/using-hadoop-streaming-for-xml-processing</id>
    <content type="html"><![CDATA[<p>In a few previous posts I talked about a project that we&#8217;re working on that involves analyzing a lot of <span class="caps">XML</span> documents from pubmed.  We&#8217;re currently not using Hadoop to parse the raw <span class="caps">XML</span>, however, due to the large number of documents in pubmed and the time it takes to do the parsing we&#8217;ve been discussing options that would allow us to scale up the processing to happen on multiple machines.  Since we&#8217;re already using Hadoop for analysis I decided to poke around a bit to see if we could figure out a way to use Hadoop for the parsing of the 617+ <span class="caps">XML</span> documents.</p>
<p>After some digging I came across <a href="http://hadoop.apache.org/common/docs/r0.15.2/streaming.html#How+do+I+parse+XML+documents+using+streaming%3F">this page</a> on the Hadoop Streaming page that said the following: &#8220;You can use the record reader StreamXmlRecordReader to process <span class="caps">XML</span> documents&#8230;.Anything found between BEGIN_STRING and END_STRING would be treated as one record for map tasks.&#8221;</p>
<p>After a few tries I wasn&#8217;t having much success, so continued to look for alternate options.  I came across Paul Ingles post on <a href="http://oobaloo.co.uk/articles/2010/1/20/processing-xml-in-hadoop.html">Processing <span class="caps">XML</span> with Hadoop</a> which pointed me to the <a href="http://github.com/apache/mahout/blob/ad84344e4055b1e6adff5779339a33fa29e1265d/examples/src/main/java/org/apache/mahout/classifier/bayes/XmlInputFormat.java">XmlInputFormat</a> class in Mahout.  I believe in order to use the XlInputFormat class from Mahout I either need to recompile Hadoop with that class included or be using a jar file for my jobs that includes that class.  Since we&#8217;re writing our mappers and reducers in Ruby I didn&#8217;t have a jar to add the class to.</p>
<p>In hopes that I was being stupid with the StreamXmlReaderRecord I decided to return to it and attempt to get it working.  After configuring it I saw some positive things in the console as I ran my job.  It did in fact look like Hadoop was breaking apart my <span class="caps">XML</span> documents into the appropriate chunks (using the start and end tags I specified in my config)</p>
<div class='bogus-wrapper'><figure class='code'><figcaption><span><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span><br />
<span class='line-number'>2</span><br />
<span class='line-number'>3</span><br />
<span class='line-number'>4</span><br />
<span class='line-number'>5</span><br />
<span class='line-number'>6</span><br />
</pre></td><td class='code'><pre><code class='ruby'>&lt;span class='line'&gt;&lt;span class="n"&gt;hadoop&lt;/span&gt; &lt;span class="n"&gt;jar&lt;/span&gt; &lt;span class="n"&gt;hadoop&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;streaming&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jar&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;   &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;input&lt;/span&gt; &lt;span class="n"&gt;medline10n0515&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xml&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;   &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;   &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;mapper&lt;/span&gt; &lt;span class="n"&gt;xml&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;mapper&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rb&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;   &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;inputreader&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;StreamXmlRecordReader,begin=&amp;lt;MedlineCitation,end=&amp;lt;/MedlineCitation&amp;gt;&amp;quot;&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;   &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;jobconf&lt;/span&gt; &lt;span class="n"&gt;mapred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tasks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;/span&gt;</code></pre></td><p></tr></table></div></figure></notextile></div></p>
<p>The next thing to figure out was how I should be retrieving the entire <span class="caps">XML</span> contents from within my mapper.  With Hadoop Streaming the input is streamed in via <span class="caps">STDIN</span> so I attempted building up the <span class="caps">XML</span> myself using some mega-smart &#8220;parse&#8221; logic!</p>
<div class='bogus-wrapper'><figure class='code'><figcaption><span><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span><br />
<span class='line-number'>2</span><br />
<span class='line-number'>3</span><br />
<span class='line-number'>4</span><br />
<span class='line-number'>5</span><br />
<span class='line-number'>6</span><br />
<span class='line-number'>7</span><br />
<span class='line-number'>8</span><br />
<span class='line-number'>9</span><br />
<span class='line-number'>10</span><br />
<span class='line-number'>11</span><br />
<span class='line-number'>12</span><br />
<span class='line-number'>13</span><br />
<span class='line-number'>14</span><br />
<span class='line-number'>15</span><br />
</pre></td><td class='code'><pre><code class='ruby'>&lt;span class='line'&gt;&lt;span class="c1"&gt;#!/usr/bin/env ruby&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;&lt;span class="n"&gt;xml&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;nil&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;&lt;span class="no"&gt;STDIN&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;each_line&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;  &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip!&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;
&lt;/span&gt;&lt;span class='line'&gt;  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;include?&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;lt;MedlineCitation&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;    &lt;span class="n"&gt;xml&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;  &lt;span class="k"&gt;else&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;    &lt;span class="n"&gt;xml&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;
&lt;/span&gt;&lt;span class='line'&gt;  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;include?&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;lt;/MedlineCitation&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;    &lt;span class="nb"&gt;puts&lt;/span&gt; &lt;span class="n"&gt;convert_to_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xml&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/span&gt;</code></pre></td><p></tr></table></div></figure></notextile></div></p>
<p>As you can see I look for the start and end tags relevant for my <span class="caps">XML</span>, and once I have a complete document I pass the <span class="caps">XML</span> to the convert_to_json method.  There&#8217;s definitely quite a bit of cleanup that can be done, as well as edge cases that aren&#8217;t handled (nested tags that match the root tag), but we&#8217;ve at least co-erced Hadoop into doing what we want.  Next up is seeing how well it works when run against the entire dataset.</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Attempts at Analyzing 19 million documents using MongoDB map/reduce]]></title>
    <link href="http://eichert12.github.com/2010/03/31/data-analysis-using-mongodb-map-reduce.html"/>
    <updated>2010-03-31T00:00:00-04:00</updated>
    <id>http://eichert12.github.com/2010/03/31/data-analysis-using-mongodb-map-reduce</id>
    <content type="html"><![CDATA[<p>Over the course of the last couple weeks we&#8217;ve been developing a system to help analyze the 19+ million documents in the pubmed database.  In my <a href="http://eichert12.github.com/2010/03/18/large-scale-data-processing-with-mongodb.html">previous post</a> I shared details about the process that we&#8217;ve been using to bring down the ~617 zipped <span class="caps">XML</span> documents that contain the articles and import them into MongoDB.  Today I&#8217;m going to share a few more details about our attempts at analyzing the pubmed database using the Map/Reduce capabilities MongoDB offers.</p>
<p>After completing the download, unzip, parse, and load steps required to get the pubmed articles into our MongoDB instance we set out to use the map/reduce capabilities in MongoDB to do analysis and aggregation.  Our initial work has focused on the keywords and <span class="caps">MESH</span> headings within pubmed articles, as well as on the relationships between authors within pubmed.  Our end goal is to have a profile for every author who has published an article in pubmed with details about what keywords and <span class="caps">MESH</span> headings appear most within the articles they publish, as well as who they commonly co-author articles with.</p>
<p>In order to build this profile we set out to write a map/reduce job to count the number of articles written by each author by keyword.  Our job writes the results of the map/reduce job to a named collection.</p>
<div class='bogus-wrapper'><figure class='code'><figcaption><span><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span><br />
<span class='line-number'>2</span><br />
<span class='line-number'>3</span><br />
<span class='line-number'>4</span><br />
<span class='line-number'>5</span><br />
<span class='line-number'>6</span><br />
<span class='line-number'>7</span><br />
<span class='line-number'>8</span><br />
<span class='line-number'>9</span><br />
<span class='line-number'>10</span><br />
</pre></td><td class='code'><pre><code class='ruby'>&lt;span class='line'&gt;&lt;span class="n"&gt;connection&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="no"&gt;Connection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mongodb&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;][&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;host&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;&lt;span class="n"&gt;db&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;connection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mongodb&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;][&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;db&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;&lt;span class="n"&gt;collection&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;collection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;articles&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;
&lt;/span&gt;&lt;span class='line'&gt;&lt;span class="n"&gt;map&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;&lt;span class="n"&gt;reduce&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;
&lt;/span&gt;&lt;span class='line'&gt;&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;collection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map_reduce&lt;/span&gt; &lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;                               &lt;span class="ss"&gt;:verbose&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="kp"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;                               &lt;span class="ss"&gt;:out&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;keywordstats&amp;quot;&lt;/span&gt;
&lt;/span&gt;</code></pre></td><p></tr></table></div></figure></notextile></div></p>
<p>The &#8220;keywordstats&#8221; map/reduce job resulted in over a half million documents being inserted into the keywordstats collection.</p>
<div class='bogus-wrapper'><figure class='code'><figcaption><span><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span><br />
<span class='line-number'>2</span><br />
<span class='line-number'>3</span><br />
<span class='line-number'>4</span><br />
<span class='line-number'>5</span><br />
<span class='line-number'>6</span><br />
<span class='line-number'>7</span><br />
<span class='line-number'>8</span><br />
<span class='line-number'>9</span><br />
</pre></td><td class='code'><pre><code class='ruby'>&lt;span class='line'&gt;&lt;span class="c1"&gt;#keywordstatus example document&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;  &lt;span class="n"&gt;_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;author_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;  &lt;span class="n"&gt;keywords&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;    &lt;span class="n"&gt;keyword1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;310&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;    &lt;span class="n"&gt;keyword2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;    &lt;span class="n"&gt;keyword3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;span class='line'&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/span&gt;</code></pre></td><p></tr></table></div></figure></notextile></div></p>
<p>The running of the keyword map/reduce analysis took approximately 30 minutes and didn&#8217;t cause us to think twice about our use of MongoDB map/reduce for our analysis.  Next we moved onto doing analysis on <span class="caps">MESH</span> headings.  Since <span class="caps">MESH</span> headings are pubmed&#8217;s official way of categorizing articles there are a lot more articles with <span class="caps">MESH</span> headings, and thus a lot more crunching for MongoDB to do.  The map/reduce jobs for the <span class="caps">MESH</span> headings were almost exactly the same as those for keywords, however, the processing took much longer due to the larger number of articles with <span class="caps">MESH</span> headings assigned.  When all was said and done MongoDB was able to process our map/reduce jobs for <span class="caps">MESH</span> headings, however, it took over 15 hours to complete (Note: we didn&#8217;t do any optimization work so its likely this could be trimmed).</p>
<p>The large increase in time required to analyze the <span class="caps">MESH</span> headings made us start to think about what other options we might consider.  However, we pressed onto our final analysis: author/co-author relationships.  Our goal with the author/co-author analysis is to be able to see who authors are co-authoring with most.  Additionally, we want to be able to create a network graph of all the authors within pubmed to do social network analysis on the graph.  In order to create the network we need to be able to figure out who has written with one another so we can create an edge between the relevant author nodes.</p>
<p>Since every article within pubmed has an author, and often multiple authors, we expected this bit of analysis  to be the most taxing on MongoDB.  Pretty soon after kicking off our author/co-author jobs we ran into problems.  Due to the large number of author/co-author relationships and the fact that a single author may co-author papers with many other authors we were unable to get our job to run without running into the memory size limitations of documents within MongoDB.</p>
<p>We evaluated other map/reduce strategies that would reduce the document size, however, the limitations that MongoDB places on the mappers and reducers prevented us from implementing those alternate strategies.  To be more specific, MongoDB requires the mapper and reducer to emit the same structure.  From the map phase we were emitting:</p>
<div>
<pre>
<code class='ruby'>author, {coauthor1: 1} #emit for each author/co-author &amp;quot;pair&amp;quot;</code>
</pre>
</div>
<p>And in our reduce phase we were consolidating all the co-author counts into a single hash to end up with:</p>
<div>
<pre>
<code class='ruby'>{
<em>id: author</em>name,
value: {
coauthor1: 31,
coauthor2: 211,
coauthor3_: 122
}
<p>}</code><br />
  </pre></p>
</div>
<p>We found that some authors had so many papers and thus so many coauthors that we were blowing past the size limitations MongoDB places on documents. An alternate strategy that we considered was changing our reduce stage to output a single author coauthor relationship with a count rather then our initial approach which reduced to an author with a hash containing all the coauthors with the counts.  However, since we can only reduce to a single output we would need to change our mapper to emit the author/co-author as the key.  Our initial attempts with this approach weren&#8217;t working well which prompted us to taken another step back to consider alternate approaches.</p>
<p>Given our needs and the amount of custom analysis we want to do against this (and other largish datasets) we decided to spend some time investigating Hadoop and Amazon Elastic Map Reduce.  Our initial experiences have been very positive, and have us feeling much more confident that the technology choice (Hadoop) won&#8217;t prevent us from exploring different types of analysis.</p>
<p>We still feel that Mongo will be a great place to persist the output of all of our Map/Reduce steps, however, we don&#8217;t feel that it&#8217;s well suited to the type of analysis that we want to do.  With Hadoop we can scale our processing quite easily, we have tremendous flexibility in what we do in both the map and reduce stages, and most importantly to us we&#8217;re using a tool that is designed specifically for the problem we&#8217;re trying to &#8220;solve&#8221;.  Mongo is a nice schema free document database with some map/reduce capabilities, however, what we need for our analysis stage is a complete map/reduce framework.  We&#8217;ll still be using Mongo, we&#8217;ll just be using it for what it&#8217;s good at and Hadoop for the rest.</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Large Scale Data Processing with MongoDB Map/Reduce (Part 1:Background)]]></title>
    <link href="http://eichert12.github.com/2010/03/18/large-scale-data-processing-with-mongodb.html"/>
    <updated>2010-03-18T00:00:00-04:00</updated>
    <id>http://eichert12.github.com/2010/03/18/large-scale-data-processing-with-mongodb</id>
    <content type="html"><![CDATA[<p>Over the course of the last week I&#8217;ve been working with a member of our team to develop a prototype data processing &#8220;engine&#8221; for analyzing articles within the <a href="http://www.ncbi.nlm.nih.gov/pubmed">pubmed database</a>.  The pubmed database consists of approximately 19 million articles that can be downloaded as approximately 617 zipped <span class="caps">XML</span> documents.</p>
<p>Our initial work has focused on downloading the complete dataset, pulling out the bits that we have interest in, and importing them into MongoDB.  For our initial analysis we&#8217;re focusing on a subset of the details available for each article.  In the future we&#8217;ll likely expand our analysis to include more details.</p>
<p>We started by downloading the 617 zipped <span class="caps">XML</span> documents from pubmed.  Once downloaded we unzipped each file, parsed out the bits that we&#8217;re interested in and saved the details in a <span class="caps">JSON</span> file optimized for importing into MongoDB. <sup class="footnote" id="fnr1"><a href="#fn1">1</a></sup></p>
<p>Once all the <span class="caps">XML</span> files were processed and the details were saved out to a <span class="caps">JSON</span> file, we used the mongoimport utility to import the <span class="caps">JSON</span> files into MongoDB.</p>
<p>The above process was run over the course of a couple days.  The most time consuming part was the parsing of the <span class="caps">XML</span> files.  We wrote <a href="http://github.com/defunkt/resque">Resque</a> workers to handle the above  so that the work could be distributed to multiple nodes running on EC2, however, I ended up running things locally so that I could test the process.  Given the pubmed database doesn&#8217;t change that often, and that we&#8217;ll rarely need to re-process the entire dataset having it run on a single machine over the course of a couple days will likely suffice.</p>
<p>After importing all the articles into MongoDB we had a pretty large MongoDB database consisting of ~18 million &#8220;documents&#8221;.  With the articles loaded into MongoDB, we moved onto the next step&#8230;<a href="http://eichert12.github.com/2010/03/31/data-analysis-using-mongodb-map-reduce.html">analyzing all 18 million documents</a>.</p>
<hr />
<p><sup class="footnote" id="fnr1"><a href="#fn1">1</a></sup> MongoDB likes a single <span class="caps">JSON</span> record on each line.<br />
<sup class="footnote" id="fnr2"><a href="#fn2">2</a></sup> This is my first blog post in ages, I need to get back into it slowly, oh so very slowly! :-)</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Network Visualization on the Web]]></title>
    <link href="http://eichert12.github.com/2009/01/30/network-visualization-on-the-web.html"/>
    <updated>2009-01-30T00:00:00-05:00</updated>
    <id>http://eichert12.github.com/2009/01/30/network-visualization-on-the-web</id>
    <content type="html"><![CDATA[<p>Over the course of the last couple months I&#8217;ve been doing quite a bit of investigation and experimentation of existing network visualization libraries.  There are a number of libraries available, some open source, some built specifically for the web, others meant for a desktop environment, some in java, others in flash, and round and round we go.</p>
<p>I&#8217;ve talked to quite a few people who have specific expertise in technologies for doing network visualization as well, ranging from flash to javascript to Silverlight to java.  My conclusion thus far is that large scale network visualizations (300+ nodes) is hard.  Once you cross the 100 node mark, you begin to have serious problems with laying out the network in a way that is usable by the user of the system that the visualization is within.  Drop on top of that the desire to make the visualization interactive (zoom, click, drag, etc), as well as the desire to have the visualization software figure out the best layout for the network itself and you have a pretty difficult problem to solve.</p>
<p>I&#8217;m currently doing some prototypes myself using Silverlight.  I don&#8217;t love the idea of using Silverlight since I doubt the penetration of Silverlight is as great as some have proclaimed, but, the advantages it offers are hard to look past.  As a long time .<span class="caps">NET</span>/C# developer I&#8217;m very comfortable with the development tools used to build Silverlight applications, as well as the language within which to do so, C#.  Silverlight appears to offer some pretty decent performance, and I suspect that it will get better as the  VM improves.  The major disadvantage of Silverlight, which I don&#8217;t know the validity of, is it&#8217;s lack of existing user base.  Since it&#8217;s relatively new, and not many sites use it, I suspect the installed base of Silverlight is much less then something like Flash.</p>
<p>The other piece of software that I&#8217;ve been spending a bit of time with is graphvis.  Graphvis is good at creating network visualizations, and supports a number of different layout algorithms.  Unfortunately the output isn&#8217;t always great, and it most certainly isn&#8217;t very interactive.  What I&#8217;m experimenting with is using graphvis to pre-compute the network layout, and then feeding that positional information into the Silverlight visualization.  The primary advantage will be that the Silverlight app won&#8217;t have to figure out the initial layout, however, it will be able to handle all the nice visualization and interactivity that&#8217;s desired.  The question still remains, is Silverlight up to the challenge?  Or is flash, processing, or a pure java applet more appropriate/capable?  Only time will tell&#8230;.</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Moving Gems from one version of Ruby Enterprise Edition to Another]]></title>
    <link href="http://eichert12.github.com/2008/12/28/moving-gems-from-one-version-of-ree-to-another.html"/>
    <updated>2008-12-28T00:00:00-05:00</updated>
    <id>http://eichert12.github.com/2008/12/28/moving-gems-from-one-version-of-ree-to-another</id>
    <content type="html"><![CDATA[<p>As mentioned in my previous post I recently built a small internal micro app with merb.  As part of the process of deploying that app I needed/wanted to update to the latest version of Ruby Enterprise Edition (<span class="caps">REE</span>) and Passenger on my slice.  One of the issues I ran into while trying to update the <span class="caps">REE</span> version is that all my old gems where not installed in my fresh new version of <span class="caps">REE</span>.  There may be a better way to accomplish this task, but the approach I ended up using was to modify this capistrano file (<a href="http://github.com/jtimberman/ree-capfile/tree/master">http://github.com/jtimberman/ree-capfile/tree/master</a>) to install the gems in the old version of <span class="caps">REE</span> in the new version.</p>
<script src="http://gist.github.com/39066.js"></script>]]></content>
  </entry>
  
</feed>
